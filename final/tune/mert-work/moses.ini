# MERT optimized configuration
# decoder /home/tomdong/tools/mosesdecoder/bin/moses
# BLEU 0.22433 on dev /home/tomdong/Documents/nlp-class/final/tune/shiji.dev.tokenized.modern
# We were before running iteration 4
# finished Thu Dec 20 00:34:16 PST 2012
### MOSES CONFIG FILE ###
#########################

# input factors
[input-factors]
0

# mapping steps
[mapping]
0 T 0

# translation tables: table type (hierarchical(0), textual (0), binary (1)), source-factors, target-factors, number of scores, file 
# OLD FORMAT is still handled for back-compatibility
# OLD FORMAT translation tables: source-factors, target-factors, number of scores, file 
# OLD FORMAT a binary table type (1) is assumed 
[ttable-file]
0 0 0 5 /home/tomdong/Documents/nlp-class/final/train/model/phrase-table.gz

# no generation models, no generation-file section

# language models: type(srilm/irstlm), factors, order, file
[lmodel-file]
8 0 3 /home/tomdong/Documents/nlp-class/final/language-model/lm.b.classical


# limit on how many phrase translations e for each phrase f are loaded
# 0 = all elements loaded
[ttable-limit]
20

# distortion (reordering) weight
[weight-d]
0.190257

# language model weights
[weight-l]
0.11236


# translation model weights
[weight-t]
0.054581
0.117545
0.145373
0.0421136
-0.143308

# no generation models, no weight-generation section

# word penalty
[weight-w]
-0.194462

[distortion-limit]
6
